{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given dataset $$S = \\{(X_1, Y_1), (X_2, Y_2), ..., (X_n, Y_N)\\}$$ we want to solve the Tikhonov minimization problem with square loss:\n",
    "\n",
    "$$min\\frac{1}{2}\\sum\\limits_{i=1}^n(f(X_i)-Y_i)^2 + \\frac{\\lambda}{2}||f||_k^2$$ where f is the regression function. According to [representer theorem](http://alex.smola.org/papers/2001/SchHerSmo01.pdf), the solution can be written as:\n",
    "\n",
    "$$f = \\sum\\limits_{i=1}^nc_ik(X_i,\\cdot)$$\n",
    "\n",
    "So the formulation can be rewritten as $$min\\frac{1}{2}\\sum\\limits_{i=1}^n(Y-Kc)^2 + \\frac{\\lambda}{2}c^tKc$$ K is the kernel matrix whose j-th element on i-th row is $k(X_i, X_j)$.\n",
    "\n",
    "The closed-form solution to this minimization problem is given as $c^* = (K+\\lambda I)^{-1}Y$. When we have a new piece of data $X^*$, the model will predict the output via $f(X^*) = \\sum ck(X_i,X^*) = Y^t(K+\\lambda I)^{-1}k(X,X^*)$.\n",
    "\n",
    "Solving the inverse of a matrix is computationally expensive. Alternatively we try to perform eigendecomposition of $K$ first and use the decomposed matrix to calculate $c$.\n",
    "\n",
    "Suppose $G = K + \\lambda I$, and the eigendecomposition of $K$ can be written as $Q\\Lambda Q^t$ where $Q$ is a orthogonal matrix, and $\\Lambda$ is diagonal. The decomposition takes $O(n^3)$ time. The inverse can be easily solved after eigendecomposition, and $c^* = Q(\\Lambda^{-1}+\\lambda I)Q^tY$.\n",
    "\n",
    "A special case is when we use linear kernel, the problem reduces to linear regression plus an extra L-2 norm regularizer. In this case the solution $c^* = (X^tX+\\lambda I)^{-1}X^tY$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to find out good penalty parameter $\\lambda$ is to apply cross validation. In practice we either define validation sets (when data is enormous) or use **leave-one-out** (when training set is small) to perform CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print len(boston.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "source": [
    "print boston.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}