{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given dataset $$S = \\{(X_1, Y_1), (X_2, Y_2), ..., (X_n, Y_N)\\}$$ we want to solve the Tikhonov minimization problem with square loss:\n",
    "\n",
    "$$min\\frac{1}{2}\\sum\\limits_{i=1}^n(f(X_i)-Y_i)^2 + \\frac{\\lambda}{2}||f||_k^2$$ where f is the regression function. According to [representer theorem](http://alex.smola.org/papers/2001/SchHerSmo01.pdf), the solution can be written as:\n",
    "\n",
    "$$f = \\sum\\limits_{i=1}^nc_ik(X_i,\\cdot)$$\n",
    "\n",
    "So the formulation can be rewritten as $$min\\frac{1}{2}\\sum\\limits_{i=1}^n(Y-Kc)^2 + \\frac{\\lambda}{2}c^tKc$$ K is the kernel matrix whose j-th element on i-th row is $k(X_i, X_j)$.\n",
    "\n",
    "The closed-form solution to this minimization problem is given as $c^* = (K+\\lambda I)^{-1}Y$. When we have a new piece of data $X^*$, the model will predict the output via $f(X^*) = \\sum ck(X_i,X^*) = Y^t(K+\\lambda I)^{-1}k(X,X^*)$.\n",
    "\n",
    "Solving the inverse of a matrix is computationally expensive. Alternatively we try to perform eigendecomposition of $K$ first and use the decomposed matrix to calculate $c$.\n",
    "\n",
    "Suppose $G = K + \\lambda I$, and the eigendecomposition of $K$ can be written as $Q\\Lambda Q^t$ where $Q$ is a orthogonal matrix, and $\\Lambda$ is diagonal. The decomposition takes $O(n^3)$ time. The inverse can be easily solved after eigendecomposition, and $c^* = Q(\\Lambda^{-1}+\\lambda I)Q^tY$.\n",
    "\n",
    "\n",
    "A special case is when we use linear kernel, the problem reduces to linear regression plus an extra L-2 norm regularizer. In this case the regression function becomes a dot product between weights $w$ and new data $X^*$, where $w = (X^tX + \\lambda I)^{-1}X^tY$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to find out good penalty parameter $\\lambda$ is to apply cross validation. In practice we either define validation sets (when data is enormous) or use **leave-one-out** (when training set is small) to perform CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "##split training, validation and testing set. only use one validation set (not CV) just to demonstrate the idea\n",
    "boston_X_train = boston.data[:-150]\n",
    "boston_X_val = boston.data[-150:50]\n",
    "boston_X_test = boston.data[-50:]\n",
    "\n",
    "boston_Y_train = boston.target[:-150]\n",
    "boston_Y_val = boston.target[-150:50]\n",
    "boston_Y_test = boston.target[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-662769.030411\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "U, s, Vt = np.linalg.svd(boston_X_train, full_matrices=False)\n",
    "temp = np.linalg.inv(np.dot(s,s) + alpha*np.eye(s.size))\n",
    "w = np.dot(np.dot(np.dot(np.dot(np.transpose(Vt), temp), s), np.transpose(U)), boston_Y_train)\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference: [technical report](http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}